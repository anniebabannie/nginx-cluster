daemon off;
user  nginx;
worker_processes 4;
error_log  /dev/stderr error;
pid        /var/run/nginx.pid;
worker_rlimit_nofile 100000;

events {
    worker_connections  5000;
    multi_accept        off;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format  main_ext  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for" '
                      '"$host" sn="$server_name" '
                      'rt=$request_time '
                      'ua="$upstream_addr" us="$upstream_status" '
                      'ut="$upstream_response_time" ul="$upstream_response_length" '
                      'cs=$upstream_cache_status' ;

    access_log  /dev/stdout main_ext;
    resolver    8.8.8.8;
   
    open_file_cache max=200000 inactive=20s;
    open_file_cache_valid 30s;
    open_file_cache_min_uses 2;
    open_file_cache_errors on;

     # don't write to disk
    proxy_request_buffering off;

    # copies data between one FD and other from within the kernel
    # faster than read() + write()
    sendfile on;

    # send headers in one piece, it is better than sending them one by one
    tcp_nopush on;

    # don't buffer data sent, good for small data bursts in real time
    tcp_nodelay on;

    # allow the server to close connection on non responding client, this will free up memory
    reset_timedout_connection on;

    # request timed out -- default 60
    client_body_timeout 10;

    # if client stop responding, free up memory -- default 60
    send_timeout 2;

    # server will close connection after this time -- default 75
    keepalive_timeout 30;

    # number of requests client can make over keep-alive -- for testing environment
    keepalive_requests 100000;

    proxy_cache_path /data/nginx-cache levels=1:2 keys_zone=http_cache:128m max_size={CACHE_SIZE} inactive=3d use_temp_path=off;

    upstream nginx-nodes {
        hash "$scheme://$host$request_uri" consistent;
        keepalive 100;

        # shard-upstream
    }

    # balance across cache servers
    server {
        listen [::]:8080;
        listen 8080 default_server;

        location / {
            access_log /dev/stdout main_ext;
            proxy_pass http://nginx-nodes/;
            proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
            proxy_next_upstream_timeout 1;
            proxy_connect_timeout 1s;
            proxy_http_version 1.1;
            proxy_set_header Connection "";
            add_header x-frontend $hostname;
        }
    }

    map $upstream_status $loggable {
        ~^[5]  1;
        default 0;
    }
    server {
        listen       [::]:8081;
        listen 8081;
        server_name  _;
        port_in_redirect        off;
        proxy_http_version      1.1;
        proxy_buffering         on;

        location /healthz {
            access_log off;
            return 200 "ok";
        }

        location / {
            # don't log requests to shards
            access_log off;
            proxy_pass https://media.giphy.com/;
            proxy_cache http_cache;

            proxy_cache_use_stale error timeout updating http_502 http_503 http_504;
            proxy_next_upstream error timeout http_500 http_502 http_503 http_504;
            proxy_cache_revalidate on;
            proxy_cache_background_update on;
            proxy_cache_lock on;
            proxy_cache_lock_timeout 1s;
            proxy_http_version 1.1;
            proxy_set_header Connection "";

            add_header Fly-Cache-Status $upstream_cache_status;
            add_header X-Instance $hostname;
        }
    }

    # health check server
    server {
        listen 8080;
        listen [::]:8080;
        server_name health.check;

        location /healthz {
            access_log off;
            return 200 "ok: healthz";
        }
    }
}